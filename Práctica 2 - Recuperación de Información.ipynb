{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e08f68",
   "metadata": {},
   "source": [
    "<div style=\"background:#5D6D7E;padding:20px;color:#ffffff;margin-top:10px;\">\n",
    "\n",
    "# NLP - Práctica 2 ( Recuperación de Información) \n",
    "\n",
    "## Profesora: Lisibonny Beato\n",
    "### Período 3-2023-2024</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cebb0323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T02:53:02.966221Z",
     "start_time": "2025-07-11T02:53:02.962963Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from os import path"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "38cb051e",
   "metadata": {},
   "source": [
    "<div style=\"background:#ff6242;padding:20px;color:#ffffff;margin-top:10px;\">\n",
    "<b>El propósito de esta asignación es que el estudiante ponga en práctica la construcción de sistemas de recuperación de información basado en el modelo clásico de RI Vector Space Model, así como también que evalue la efectividad de estos modelos mediante el uso de una colección de referencia (Benchmark).\n",
    "<br />\n",
    "<br />\n",
    "Para esta práctica estará utilizando el siguiente repositorio:https://github.com/oussbenk/cranfield-trec-dataset. En el mismo encontrarán los archivos  cran.all.1400.xml, cran.qry.xml, cranqrel.trec.txt:\n",
    "<ul>\n",
    "<li>cran.all.1400.xml contiene 1,400 resumenes de artículos científicos.</li>\n",
    "<li>cran.qry.xml 225 términos que representan consultas.</li>\n",
    "<li>cranqrel.trec.txt contiene los juicios de relevancia a dichas consultas.</li>\n",
    "    </ul>  \n",
    "<br />\n",
    "<br />\n",
    "Estudie en detalle la estructura y el contenido de este conjunto de documentos provistos antes de comenzar.    \n",
    "<br />\n",
    "<br />\n",
    "En este trabajo, aparte del código, debe proveer una interpretación para cada tarea y un análisis para cada resultado obtenido que así lo amerite.</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebd3b5",
   "metadata": {},
   "source": [
    "## 1. Ejercicio 1\n",
    "### Puntuación máxima de la tarea: 3 puntos\n",
    "#### Limpieza y preparación de los datos, utilizando distintas técnicas de las ya vistas en clases. Para esta tarea utilizará el archivo cran.all.1400.xml, específicamente sus columnas title y text.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T02:53:03.153615Z",
     "start_time": "2025-07-11T02:53:02.981310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "with open('cranfield-trec-dataset/cran.all.1400.xml', 'r') as f:   # Reading file\n",
    "    xml = f.read()\n",
    "\n",
    "xml = '<root>' + xml + '</root>'   # Let's add a root tag\n",
    "\n",
    "root = ElementTree.fromstring(xml)\n",
    "\n",
    "# Simple loop through each document\n",
    "df = pd.DataFrame(columns=['docno', 'title', 'author', 'bib', 'text'])\n",
    "\n",
    "for doc in root:\n",
    "    docno = doc.find('docno').text.strip() if doc.find('docno').text is not None else ''\n",
    "    title = doc.find('title').text.strip() if doc.find('title').text is not None else ''\n",
    "    author = doc.find('author').text.strip() if doc.find('author').text is not None else ''\n",
    "    bib = doc.find('bib').text.strip() if doc.find('bib').text is not None else ''\n",
    "    text = doc.find('text').text.strip() if doc.find('text').text is not None else ''\n",
    "\n",
    "    new_row = pd.DataFrame({\n",
    "        'docno': [docno],\n",
    "        'title': [title],\n",
    "        'author': [author],\n",
    "        'bib': [bib],\n",
    "        'text': [text]\n",
    "    })\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df"
   ],
   "id": "23dd537f22193b34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     docno                                              title  \\\n",
       "0        1  experimental investigation of the aerodynamics...   \n",
       "1        2  simple shear flow past a flat plate in an inco...   \n",
       "2        3  the boundary layer in simple shear flow past a...   \n",
       "3        4  approximate solutions of the incompressible la...   \n",
       "4        5  one-dimensional transient heat conduction into...   \n",
       "...    ...                                                ...   \n",
       "1395  1396  shear buckling of clamped and simply-supported...   \n",
       "1396  1397  critical shear stress of an infinitely long si...   \n",
       "1397  1398  stability of rectangular plates under shear an...   \n",
       "1398  1399  buckling of transverse stiffened plates under ...   \n",
       "1399  1400  the buckling shear stress of simply-supported ...   \n",
       "\n",
       "                         author  \\\n",
       "0                  brenckman,m.   \n",
       "1                     ting-yili   \n",
       "2                 m. b. glauert   \n",
       "3                      yen,k.t.   \n",
       "4                  wasserman,b.   \n",
       "...                         ...   \n",
       "1395  cook,i.t. and rockey,k.c.   \n",
       "1396  stein,m. and fralich,r.w.   \n",
       "1397                     way,s.   \n",
       "1398                  wang,t.k.   \n",
       "1399               kleeman,p.w.   \n",
       "\n",
       "                                                    bib  \\\n",
       "0                            j. ae. scs. 25, 1958, 324.   \n",
       "1     department of aeronautical engineering, rensse...   \n",
       "2     department of mathematics, university of manch...   \n",
       "3                            j. ae. scs. 22, 1955, 728.   \n",
       "4                            j. ae. scs. 24, 1957, 924.   \n",
       "...                                                 ...   \n",
       "1395                         aero. quart. 13, 1962, 41.   \n",
       "1396                                naca tn.1851, 1949.   \n",
       "1397                       j. app. mech. 3, 1936, a131.   \n",
       "1398                          j.app.mech. 3,1947, a269.   \n",
       "1399                              arc r + m.2971, 1953.   \n",
       "\n",
       "                                                   text  \n",
       "0     experimental investigation of the aerodynamics...  \n",
       "1     simple shear flow past a flat plate in an inco...  \n",
       "2     the boundary layer in simple shear flow past a...  \n",
       "3     approximate solutions of the incompressible la...  \n",
       "4     one-dimensional transient heat conduction into...  \n",
       "...                                                 ...  \n",
       "1395  shear buckling of clamped and simply-supported...  \n",
       "1396  critical shear stress of an infinitely long si...  \n",
       "1397  stability of rectangular plates under shear an...  \n",
       "1398  buckling of transverse stiffened plates under ...  \n",
       "1399  the buckling shear stress of simply-supported ...  \n",
       "\n",
       "[1400 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>bib</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "      <td>brenckman,m.</td>\n",
       "      <td>j. ae. scs. 25, 1958, 324.</td>\n",
       "      <td>experimental investigation of the aerodynamics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "      <td>ting-yili</td>\n",
       "      <td>department of aeronautical engineering, rensse...</td>\n",
       "      <td>simple shear flow past a flat plate in an inco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "      <td>m. b. glauert</td>\n",
       "      <td>department of mathematics, university of manch...</td>\n",
       "      <td>the boundary layer in simple shear flow past a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "      <td>yen,k.t.</td>\n",
       "      <td>j. ae. scs. 22, 1955, 728.</td>\n",
       "      <td>approximate solutions of the incompressible la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "      <td>wasserman,b.</td>\n",
       "      <td>j. ae. scs. 24, 1957, 924.</td>\n",
       "      <td>one-dimensional transient heat conduction into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>1396</td>\n",
       "      <td>shear buckling of clamped and simply-supported...</td>\n",
       "      <td>cook,i.t. and rockey,k.c.</td>\n",
       "      <td>aero. quart. 13, 1962, 41.</td>\n",
       "      <td>shear buckling of clamped and simply-supported...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1397</td>\n",
       "      <td>critical shear stress of an infinitely long si...</td>\n",
       "      <td>stein,m. and fralich,r.w.</td>\n",
       "      <td>naca tn.1851, 1949.</td>\n",
       "      <td>critical shear stress of an infinitely long si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>1398</td>\n",
       "      <td>stability of rectangular plates under shear an...</td>\n",
       "      <td>way,s.</td>\n",
       "      <td>j. app. mech. 3, 1936, a131.</td>\n",
       "      <td>stability of rectangular plates under shear an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1399</td>\n",
       "      <td>buckling of transverse stiffened plates under ...</td>\n",
       "      <td>wang,t.k.</td>\n",
       "      <td>j.app.mech. 3,1947, a269.</td>\n",
       "      <td>buckling of transverse stiffened plates under ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1400</td>\n",
       "      <td>the buckling shear stress of simply-supported ...</td>\n",
       "      <td>kleeman,p.w.</td>\n",
       "      <td>arc r + m.2971, 1953.</td>\n",
       "      <td>the buckling shear stress of simply-supported ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1f70b865",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Utilice esta celda para colocar comentarios en el notebook, cuando lo estime necesario. Copiela varias veces donde considere.\n",
    "</div>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T02:53:03.509376Z",
     "start_time": "2025-07-11T02:53:03.182243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ],
   "id": "f2e245f194fd2fe2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1020)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1020)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T02:53:03.540356Z",
     "start_time": "2025-07-11T02:53:03.536663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_cleaning(df):\n",
    "    # Convert to lowercase\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    df['title'] = df['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "    # Tokenization\n",
    "    df['title'] = df['title'].apply(word_tokenize)\n",
    "    df['text'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['title'] = df['title'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    #Lemmatization\n",
    "    df['title'] = df['title'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x))])\n",
    "    df['text'] = df['text'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x))])\n",
    "\n",
    "    return df"
   ],
   "id": "3cded075120e5933",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T02:53:03.682244Z",
     "start_time": "2025-07-11T02:53:03.568622Z"
    }
   },
   "cell_type": "code",
   "source": "df = data_cleaning(df)",
   "id": "cf039e52569c4bcf",
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt_tab\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt_tab/english/\u001B[0m\n\n  Searched in:\n    - '/Users/cristiandelahoz/nltk_data'\n    - '/Users/cristiandelahoz/Developer/nlp/.venv/nltk_data'\n    - '/Users/cristiandelahoz/Developer/nlp/.venv/share/nltk_data'\n    - '/Users/cristiandelahoz/Developer/nlp/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mLookupError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df = \u001B[43mdata_cleaning\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mdata_cleaning\u001B[39m\u001B[34m(df)\u001B[39m\n\u001B[32m      8\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m] = df[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m].apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: re.sub(\u001B[33mr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m[^\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms]\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m, x))\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# Tokenization\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtitle\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword_tokenize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m] = df[\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m].apply(word_tokenize)\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Remove stopwords\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/pandas/core/series.py:4935\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4800\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4801\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4802\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4807\u001B[39m     **kwargs,\n\u001B[32m   4808\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4809\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4810\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4811\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4926\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4927\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4928\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4929\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4930\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4931\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4932\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4933\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4934\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m4935\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1422\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1419\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1421\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1422\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1502\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1496\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1497\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1498\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1499\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1500\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1501\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1502\u001B[39m mapped = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1503\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[32m   1504\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1506\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1507\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1508\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1509\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/pandas/core/base.py:925\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    922\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    923\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/lib.pyx:2999\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001B[39m, in \u001B[36mword_tokenize\u001B[39m\u001B[34m(text, language, preserve_line)\u001B[39m\n\u001B[32m    127\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mword_tokenize\u001B[39m(text, language=\u001B[33m\"\u001B[39m\u001B[33menglish\u001B[39m\u001B[33m\"\u001B[39m, preserve_line=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m    128\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    129\u001B[39m \u001B[33;03m    Return a tokenized copy of *text*,\u001B[39;00m\n\u001B[32m    130\u001B[39m \u001B[33;03m    using NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    140\u001B[39m \u001B[33;03m    :type preserve_line: bool\u001B[39;00m\n\u001B[32m    141\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m     sentences = [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    143\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[32m    144\u001B[39m         token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001B[32m    145\u001B[39m     ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001B[39m, in \u001B[36msent_tokenize\u001B[39m\u001B[34m(text, language)\u001B[39m\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msent_tokenize\u001B[39m(text, language=\u001B[33m\"\u001B[39m\u001B[33menglish\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    110\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[33;03m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[32m    112\u001B[39m \u001B[33;03m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    117\u001B[39m \u001B[33;03m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[32m    118\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     tokenizer = \u001B[43m_get_punkt_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer.tokenize(text)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001B[39m, in \u001B[36m_get_punkt_tokenizer\u001B[39m\u001B[34m(language)\u001B[39m\n\u001B[32m     96\u001B[39m \u001B[38;5;129m@functools\u001B[39m.lru_cache\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_get_punkt_tokenizer\u001B[39m(language=\u001B[33m\"\u001B[39m\u001B[33menglish\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m     98\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     99\u001B[39m \u001B[33;03m    A constructor for the PunktTokenizer that utilizes\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[33;03m    a lru cache for performance.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    103\u001B[39m \u001B[33;03m    :type language: str\u001B[39;00m\n\u001B[32m    104\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m105\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPunktTokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001B[39m, in \u001B[36mPunktTokenizer.__init__\u001B[39m\u001B[34m(self, lang)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, lang=\u001B[33m\"\u001B[39m\u001B[33menglish\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1743\u001B[39m     PunktSentenceTokenizer.\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1744\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mload_lang\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001B[39m, in \u001B[36mPunktTokenizer.load_lang\u001B[39m\u001B[34m(self, lang)\u001B[39m\n\u001B[32m   1746\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_lang\u001B[39m(\u001B[38;5;28mself\u001B[39m, lang=\u001B[33m\"\u001B[39m\u001B[33menglish\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1747\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnltk\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m find\n\u001B[32m-> \u001B[39m\u001B[32m1749\u001B[39m     lang_dir = \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtokenizers/punkt_tab/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlang\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m/\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1750\u001B[39m     \u001B[38;5;28mself\u001B[39m._params = load_punkt_params(lang_dir)\n\u001B[32m   1751\u001B[39m     \u001B[38;5;28mself\u001B[39m._lang = lang\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/nlp/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001B[39m, in \u001B[36mfind\u001B[39m\u001B[34m(resource_name, paths)\u001B[39m\n\u001B[32m    577\u001B[39m sep = \u001B[33m\"\u001B[39m\u001B[33m*\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m70\u001B[39m\n\u001B[32m    578\u001B[39m resource_not_found = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m579\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[31mLookupError\u001B[39m: \n**********************************************************************\n  Resource \u001B[93mpunkt_tab\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt_tab/english/\u001B[0m\n\n  Searched in:\n    - '/Users/cristiandelahoz/nltk_data'\n    - '/Users/cristiandelahoz/Developer/nlp/.venv/nltk_data'\n    - '/Users/cristiandelahoz/Developer/nlp/.venv/share/nltk_data'\n    - '/Users/cristiandelahoz/Developer/nlp/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "36d2ba20",
   "metadata": {},
   "source": [
    "## 2. Ejercicio 2\n",
    "### Puntuación máxima de la tarea: 2 puntos\n",
    "#### Construir al menos dos modelos de recuperación de información basado en el modelo vectorial, visto en detalle en clases, cada uno con una configuración distinta de textos, tal y como se indica a continuación: solo title y title+text."
   ]
  },
  {
   "cell_type": "code",
   "id": "44dc13eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T02:53:03.686544Z",
     "start_time": "2025-07-03T16:00:56.746097Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d803e21",
   "metadata": {},
   "source": [
    "## 3. Ejercicio 3\n",
    "### Puntuación máxima de la tarea: 5 puntos\n",
    "#### Evaluar los modelos de recuperación de información construidos, mediante la  utilización de la colección de referencia dada (Archivos cran.qry.xml y cranqrel.trec.txt  ).  Seleccione dos métricas ampliamente utilizadas en la evaluación de este tipo de sistemas y analice los resultados para determinar cual es el mejor de los modelos. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ec06e60",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
