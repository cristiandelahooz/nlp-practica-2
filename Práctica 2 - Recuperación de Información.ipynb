{
 "cells": [
  {
   "cell_type": "code",
   "id": "cebb0323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:01:04.804127Z",
     "start_time": "2025-07-11T07:01:04.580093Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1020)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1020)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1020)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "id": "38cb051e",
   "metadata": {},
   "source": [
    "<div style=\"background:#ff6242;padding:20px;color:#ffffff;margin-top:10px;\">\n",
    "<b>El propósito de esta asignación es que el estudiante ponga en práctica la construcción de sistemas de recuperación de información basado en el modelo clásico de RI Vector Space Model, así como también que evalue la efectividad de estos modelos mediante el uso de una colección de referencia (Benchmark).\n",
    "<br />\n",
    "<br />\n",
    "Para esta práctica estará utilizando el siguiente repositorio:https://github.com/oussbenk/cranfield-trec-dataset. En el mismo encontrarán los archivos  cran.all.1400.xml, cran.qry.xml, cranqrel.trec.txt:\n",
    "<ul>\n",
    "<li>cran.all.1400.xml contiene 1,400 resumenes de artículos científicos.</li>\n",
    "<li>cran.qry.xml 225 términos que representan consultas.</li>\n",
    "<li>cranqrel.trec.txt contiene los juicios de relevancia a dichas consultas.</li>\n",
    "    </ul>  \n",
    "<br />\n",
    "<br />\n",
    "Estudie en detalle la estructura y el contenido de este conjunto de documentos provistos antes de comenzar.    \n",
    "<br />\n",
    "<br />\n",
    "En este trabajo, aparte del código, debe proveer una interpretación para cada tarea y un análisis para cada resultado obtenido que así lo amerite.</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebd3b5",
   "metadata": {},
   "source": [
    "## 1. Ejercicio 1\n",
    "### Puntuación máxima de la tarea: 3 puntos\n",
    "#### Limpieza y preparación de los datos, utilizando distintas técnicas de las ya vistas en clases. Para esta tarea utilizará el archivo cran.all.1400.xml, específicamente sus columnas title y text.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:01:04.845242Z",
     "start_time": "2025-07-11T07:01:04.841845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wrap_trec_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        xml_content = f.read()\n",
    "    return '<root>' + xml_content + '</root>'"
   ],
   "id": "73d6748718111747",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:01:05.028551Z",
     "start_time": "2025-07-11T07:01:04.864802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xml.etree.ElementTree as ElementTree\n",
    "def load_documents():\n",
    "    xml = wrap_trec_file('cranfield-trec-dataset/cran.all.1400.xml')\n",
    "    root = ElementTree.fromstring(xml)\n",
    "    df = pd.DataFrame(columns=['docno', 'title', 'author', 'bib', 'text'])\n",
    "    for doc in root:\n",
    "        docno = doc.find('docno').text.strip() if doc.find('docno').text is not None else ''\n",
    "        title = doc.find('title').text.strip() if doc.find('title').text is not None else ''\n",
    "        author = doc.find('author').text.strip() if doc.find('author').text is not None else ''\n",
    "        bib = doc.find('bib').text.strip() if doc.find('bib').text is not None else ''\n",
    "        text = doc.find('text').text.strip() if doc.find('text').text is not None else ''\n",
    "        new_row = pd.DataFrame({'docno': [docno], 'title': [title], 'author': [author], 'bib': [bib], 'text': [text]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    return df\n",
    "docs = load_documents()"
   ],
   "id": "23dd537f22193b34",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:01:05.044195Z",
     "start_time": "2025-07-11T07:01:05.040808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_queries(query_file):\n",
    "    tree = ElementTree.parse(query_file)\n",
    "    root = tree.getroot()\n",
    "    queries = []\n",
    "    query_ids = []\n",
    "    for topic in root.findall('top'):\n",
    "        qid = topic.find('num').text.strip()\n",
    "        title = topic.find('title').text\n",
    "        queries.append(title)\n",
    "        query_ids.append(qid)\n",
    "    return query_ids, queries\n",
    "query_ids, queries = load_queries('cranfield-trec-dataset/cran.qry.xml')"
   ],
   "id": "46c8bfeb9e8128f8",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:01:05.296106Z",
     "start_time": "2025-07-11T07:01:05.061291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "id": "f2e245f194fd2fe2",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:01:25.543184Z",
     "start_time": "2025-07-11T07:01:05.306650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_cleaning(dataframe):\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['text'] = dataframe['text'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    dataframe['text'] = dataframe['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    dataframe['title'] = dataframe['title'].apply(word_tokenize)\n",
    "    dataframe['text'] = dataframe['text'].apply(word_tokenize)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dataframe['title'] = dataframe['title'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    dataframe['text'] = dataframe['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    dataframe['title'] = dataframe['title'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x))])\n",
    "    dataframe['text'] = dataframe['text'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x))])\n",
    "    return dataframe\n",
    "docs = data_cleaning(docs)"
   ],
   "id": "3cded075120e5933",
   "outputs": [],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "id": "36d2ba20",
   "metadata": {},
   "source": [
    "## 2. Ejercicio 2 y 3: Modelos y Evaluación\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:04:44.101573Z",
     "start_time": "2025-07-11T07:04:42.090529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def vectorize(docs, queries):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85)\n",
    "    doc_vectors = vectorizer.fit_transform(docs)\n",
    "    query_vectors = vectorizer.transform(queries)\n",
    "    return doc_vectors, query_vectors, vectorizer\n",
    "\n",
    "def rank_documents(doc_vectors, query_vector, doc_ids):\n",
    "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    ranked_indices = np.argsort(similarities)[::-1]\n",
    "    return [(doc_ids[i], similarities[i]) for i in ranked_indices]\n",
    "\n",
    "def save_trec_results(query_id, ranked_docs, run_name, output_file):\n",
    "    with open(output_file, 'a') as f:\n",
    "        for rank, (doc_id, score) in enumerate(ranked_docs[:100], 1):\n",
    "            f.write(f\"{query_id} Q0 {doc_id} {rank} {score:.4f} {run_name}\\n\")\n",
    "\n",
    "def expand_query_with_synsets(query):\n",
    "    expanded_query_words = []\n",
    "    for word in query.split():\n",
    "        expanded_query_words.append(word)\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if synsets:\n",
    "            for lemma in synsets[0].lemmas():\n",
    "                lemma_name = lemma.name().replace('_', ' ')\n",
    "                if lemma_name not in expanded_query_words:\n",
    "                    expanded_query_words.append(lemma_name)\n",
    "    return ' '.join(expanded_query_words)\n",
    "\n",
    "def clean_and_expand_queries(queries):\n",
    "    cleaned_queries = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for q in queries:\n",
    "        q = q.lower()\n",
    "        q = re.sub(r'[^\\w\\s]', '', q)\n",
    "        tokens = word_tokenize(q)\n",
    "        q_words = [word for word in tokens if word not in stop_words]\n",
    "        lemmatized = [token.lemma_ for token in nlp(' '.join(q_words))]\n",
    "        final_query = ' '.join(lemmatized)\n",
    "        expanded_query = expand_query_with_synsets(final_query)\n",
    "        cleaned_queries.append(expanded_query)\n",
    "    return cleaned_queries\n",
    "\n",
    "expanded_queries = clean_and_expand_queries(queries)"
   ],
   "id": "b58fff7fa78ee3b6",
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "source": [
    "# Modelo 1: Title-Only (con Expansión de Consulta)\n",
    "docs_title = docs['title'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "doc_vectors_title, query_vectors_title, _ = vectorize(docs_title, expanded_queries)\n",
    "output_file_title = \"trec_results_title.txt\"\n",
    "if os.path.exists(output_file_title): os.remove(output_file_title)\n",
    "for i, (qid, query_vector) in enumerate(zip(query_ids, query_vectors_title)):\n",
    "    ranked_docs = rank_documents(doc_vectors_title, query_vector, docs['docno'])\n",
    "    save_trec_results(qid, ranked_docs, \"title_expanded\", output_file_title)\n",
    "\n",
    "# Modelo 2: Title + Text (con Expansión de Consulta)\n",
    "docs['title_text'] = docs.apply(lambda row: ' '.join(row['title']) + ' ' + ' '.join(row['text']), axis=1)\n",
    "docs_title_text = docs['title_text']\n",
    "doc_vectors_tt, query_vectors_tt, _ = vectorize(docs_title_text, expanded_queries)\n",
    "output_file_tt = \"trec_results_title_text.txt\"\n",
    "if os.path.exists(output_file_tt): os.remove(output_file_tt)\n",
    "for i, (qid, query_vector) in enumerate(zip(query_ids, query_vectors_tt)):\n",
    "    ranked_docs = rank_documents(doc_vectors_tt, query_vector, docs['docno'])\n",
    "    save_trec_results(qid, ranked_docs, \"title_text_expanded\", output_file_tt)"
   ],
   "id": "run-models-cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T07:04:47.373985Z",
     "start_time": "2025-07-11T07:04:46.444715Z"
    }
   },
   "outputs": [],
   "execution_count": 105
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
